# NOTES
# You will need 4 concurrent pipelines to allow this to run as fast as possible
# Please generate an API Key Codefresh CLI https://g.codefresh.io/account/tokens and store in API_KEY variable
# You need to configure Docker Registry access in Codefresh and set push build step for (3) micro-services to push to that registry by friendly name
# You need to configure Kubernetes with a Pull Secret for the Registry and update variable KUBE_PULL_SECRET with the name you gave the secret
# Pull Secrets explained: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
# You can use your integrated Codefresh Registry for above just follow steps documented here: https://codefresh.io/docs/docs/docker-registries/codefresh-registry/
# Update the _repository values below with that value ex. r.cfcr.io/CF_ACCOUNT/CF_REPOSITORY/PIPELINE_NAME or r.cfcr.io/perfect_pipeline/codefresh/example-voting-app-worker
# Please make sure to update each pipeline yaml file under result, vote and worker with your correct friendly Docker Registry name See comment inline on those files
# Pipeline IDs for the pipelines for 3 microservices (Shown in curl webhook command on pipeline page in Configuration > General Settings)
# Create KUBE_CONTEXT variables with Codefresh friendly name given to your Kubernetes Cluster
# Create KUBE_NAMESPACE variable with Kubernetes namespace for Release
# Create HELM_RELEASE_NAME variables with Helm Release Name
# Create BROWSERS variable with space delimited supported browsers 'chrome firefox'
version: '1.0'
steps:
  BuildingMicroServices:
    title: Executing Parallel Docker Builds
    image: 'codefresh/cli:latest'
    commands: 
      - apk add bash
      - codefresh auth create-context --api-key ${{CODEFRESH_CLI_KEY}}
      - bash -c 'IFS=" " read -a pipelineidsarray <<< "${{PARALLEL_PIPELINES_IDS}}" && for id in "${pipelineidsarray[@]}"; do buildids="$buildids $(codefresh run $id -b=${{CF_BRANCH}} -d)"; done && codefresh wait $buildids'
  HelmUpgrade:
    title: Running Helm Upgrade
    image: codefresh/cfstep-helm:2.8.0 # You need to make sure you've set this to the correct version of Tiller installed on your helm init
    working_directory: ./
    environment:
      - CHART_NAME=${{HELM_CHART_NAME}}
      - RELEASE_NAME=${{HELM_RELEASE_NAME}} # Updated for Seleniun
      - KUBE_CONTEXT=${{KUBE_CONTEXT}}
      - NAMESPACE=${{KUBE_NAMESPACE}}
      - DEBUG_CHART=true
      - custom_result_image_repository=r.cfcr.io/${{CODEFRESH_ACCOUNT}}/codefresh/example-voting-app-result
      - custom_result_image_tag=${{CF_BRANCH_TAG_NORMALIZED}}-${{CF_SHORT_REVISION}}
      - custom_result_image_pullSecret=${{KUBE_PULL_SECRET}}
      - custom_vote_image_repository=r.cfcr.io/${{CODEFRESH_ACCOUNT}}/codefresh/example-voting-app-vote
      - custom_vote_image_tag=${{CF_BRANCH_TAG_NORMALIZED}}-${{CF_SHORT_REVISION}}
      - custom_vote_image_pullSecret=${{KUBE_PULL_SECRET}}
      - custom_worker_image_repository=r.cfcr.io/${{CODEFRESH_ACCOUNT}}/codefresh/example-voting-app-worker
      - custom_worker_image_tag=${{CF_BRANCH_TAG_NORMALIZED}}-${{CF_SHORT_REVISION}}
      - custom_worker_image_pullSecret=${{KUBE_PULL_SECRET}}
  GetKubernetesServicesEndpoints:
    title: Getting Kubernetes Services Endpoints
    image: codefresh/cfstep-helm:2.8.0
    commands:
      - bash -c 'IFS=" " read -a services <<< "${{SERVICES}}" && for service in "${services[@]}"; do external_ip=""; while [ -z $external_ip ]; do echo "Waiting for end point..."; external_ip=$(kubectl get svc ${{HELM_RELEASE_NAME}}-${service} --namespace ${{KUBE_NAMESPACE}} --template="{{range .status.loadBalancer.ingress}}{{.ip}}{{end}}"); [ -z "$external_ip" ] && sleep 10; done; echo "End point ready-" && echo $external_ip; cf_export ${service^^}_ENDPOINT_IP=$external_ip; done'
  BuildingTestDockerImage:
    title: Building Test Docker Image
    type: build
    image_name: codefresh/example-voting-app-tests
    working_directory: ./
    dockerfile: Dockerfile
    tag: '${{CF_BRANCH_TAG_NORMALIZED}}-${{CF_SHORT_REVISION}}'
  PostDeploymentVerificationTests:
    title: Running Selenium DVTs
    type: composition
    composition:
      version: '2'
      services:
        selenium_hub:
          image: selenium/hub
          ports:
            - 4444
          environment:
            - SE_OPTS=-debug
            - GRID_MAX_SESSION=5
        chrome_node:
          image: selenium/node-chrome
          ports:
            - 5900
            - 5555
          command: bash -c "sleep 5 && /opt/bin/entry_point.sh"
          depends_on: 
            - selenium_hub
          environment:
            - HUB_HOST=selenium_hub
            - REMOTE_HOST=http://chrome_node:5555
            - NODE_MAX_SESSION=5
            - NODE_MAX_INSTANCES=5
        firefox_node:
          image: selenium/node-firefox
          ports:
            - 5900
            - 5555
          command: bash -c "sleep 5 && /opt/bin/entry_point.sh"
          depends_on: 
            - selenium_hub
          environment:
            - HUB_HOST=selenium_hub
            - REMOTE_HOST=http://firefox_node:5555
            - NODE_MAX_SESSION=5
            - NODE_MAX_INSTANCES=5
    composition_candidates:
      test:
        image: ${{BuildingTestDockerImage}}
        working_dir: ${{CF_VOLUME_PATH}}/${{CF_REPO_NAME}}
        environment:
          VOTE_ENDPOINT_IP: ${{VOTE_ENDPOINT_IP}}
          RESULT_ENDPOINT_IP: ${{RESULT_ENDPOINT_IP}}
        command: bash -c 'IFS=" " read -a browserarray <<< "${{BROWSERS}}" && for browser in "$${browserarray[@]}"; do BROWSER=$$browser python -m pytest -vvv --html=./selenium-report-$${browser}.html --self-contained-html ./tests/selenium/test_app.py; done'
        volumes:
          - '${{CF_VOLUME_NAME}}:/codefresh/volume'
    add_flow_volume_to_composition: true
    on_success:
      metadata:
        set:
          - '${{BuildingTestDockerImage.imageId}}':
              - SELENIUM_DVTS: true
    on_fail:
      metadata:
        set:
          - '${{BuildingTestDockerImage.imageId}}':
              - SELENIUM_DVTS: false
  ArchiveSeleniumDVTReports:
    title: Archiving Selenium DVT Reports
    image: mesosphere/aws-cli
    working_directory: ./
    commands:
      - apk update
      - apk upgrade
      - apk add bash
      - bash -c 'IFS=" " read -a browserarray <<< "${{BROWSERS}}" && for browser in "${browserarray[@]}"; do BROWSER=$browser aws s3 cp ./selenium-report-${browser}.html s3://${{S3_BUCKETNAME}}/${{CF_BUILD_ID}}/selenium-report-${browser}.html --acl public-read; done'
    on_success:
     metadata:
        set:
          # I manually setup metadata for each browser
          - ${{BuildingTestDockerImage.imageId}}:
              - CHROME_SELENIUM_DVTS: "https://s3.${{AWS_DEFAULT_REGION}}.amazonaws.com/${{S3_BUCKETNAME}}/${{CF_BUILD_ID}}/selenium-report-chrome.html"
              - FIREFOX_SELENIUM_DVTS: "https://s3.${{AWS_DEFAULT_REGION}}.amazonaws.com/${{S3_BUCKETNAME}}/${{CF_BUILD_ID}}/selenium-report-firefox.html"
  PushChartToHelmRepository:
    title: Pushing Chart to Codefresh Chart Museum
    image: codefresh/cfstep-helm:2.9.0
    environment:
      - ACTION=push
      - CHART_REF=${{HELM_CHART_NAME}}